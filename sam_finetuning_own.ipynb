{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09b453c",
   "metadata": {},
   "source": [
    "# Finetuning Segment Anything with `µsam`\n",
    "\n",
    "This notebook shows how to use Segment Anything for Microscopy to fine-tune a Segment Anything Model (SAM) to segment brightfield images of spheroids \n",
    "\n",
    "This notebook has been modified based on the notebook provided along with the micro-sam software (https://github.com/computational-cell-analytics/micro-sam)\n",
    "https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb v1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367c9ce-896b-4d22-bce4-ebad84fdfb1c",
   "metadata": {},
   "source": [
    "## Load required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7186f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import FileLink\n",
    "\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import label as connected_components\n",
    "from skimage import measure, morphology, segmentation\n",
    "from skimage.measure import regionprops, label\n",
    "import torch\n",
    "import math\n",
    "import gc\n",
    "from contextlib import nullcontext\n",
    "import pandas as pd\n",
    "import math\n",
    "from roifile import ImagejRoi, roiwrite\n",
    "from scipy import ndimage\n",
    "import re\n",
    "import torch_em\n",
    "from torch_em.model import UNETR\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch_em.loss import DiceBasedDistanceLoss\n",
    "from torch_em.util.util import get_random_colors\n",
    "from torch_em.transform.label import PerObjectDistanceTransform\n",
    "\n",
    "from micro_sam import util\n",
    "import micro_sam.training as sam_training\n",
    "from micro_sam.sample_data import fetch_tracking_example_data, fetch_tracking_segmentation_data\n",
    "from micro_sam.instance_segmentation import (\n",
    "    InstanceSegmentationWithDecoder,\n",
    "    get_predictor_and_decoder,\n",
    "    mask_data_to_segmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8921fd-60bc-43fa-b376-7e99a546cb2e",
   "metadata": {},
   "source": [
    "## Define required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c324186-0b7f-4072-9fea-ea7774175717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelTransform:\n",
    "    def __init__(self, train_instance_segmentation):\n",
    "        self.train_instance_segmentation = train_instance_segmentation\n",
    "        \n",
    "    def __call__(self, labels):\n",
    "        if self.train_instance_segmentation:\n",
    "            # Computes the distance transform for objects to jointly perform the additional decoder-based automatic instance segmentation (AIS) and finetune Segment Anything.\n",
    "            label_transform = PerObjectDistanceTransform(\n",
    "                distances=True,\n",
    "                boundary_distances=True,\n",
    "                directed_distances=False,\n",
    "                foreground=True,\n",
    "                instances=True,\n",
    "                min_size=25\n",
    "            )\n",
    "        else:\n",
    "            # Ensures the individual object instances.to finetune the clasiscal Segment Anything.\n",
    "            label_transform = torch_em.transform.label.connected_components\n",
    "\n",
    "        labels = label_transform(labels)\n",
    "        return labels\n",
    "\n",
    "class CudaMemoryManager:\n",
    "    def __enter__(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "#Function for the AIS\n",
    "def run_automatic_instance_segmentation(image, checkpoint_path, model_type=\"vit_b_lm\", device=None):\n",
    "    \"\"\"Automatic Instance Segmentation by training an additional instance decoder in SAM.\n",
    "\n",
    "    NOTE: It is supported only for `µsam` models.\n",
    "    \n",
    "    Args:\n",
    "        image: The input image.\n",
    "        model_type: The choice of the `µsam` model.\n",
    "        \n",
    "    Returns:\n",
    "        The instance segmentation.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the model attributes using the finetuned µsam model weights.\n",
    "    #   - the 'predictor' object for generating predictions using the Segment Anything model.\n",
    "    #   - the 'decoder' backbone (for AIS).\n",
    "    predictor, decoder = get_predictor_and_decoder(\n",
    "        model_type=model_type,  # choice of the Segment Anything model\n",
    "        checkpoint_path=checkpoint_path,  # overwrite to pass our own finetuned model\n",
    "        device=device,  # the device to run the model inference\n",
    "    )\n",
    "    \n",
    "    # Step 2: Computation of the image embeddings from the vision transformer-based image encoder.\n",
    "    image_embeddings = util.precompute_image_embeddings(\n",
    "        predictor=predictor,  # the predictor object responsible for generating predictions\n",
    "        input_=image,  # the input image\n",
    "        ndim=2,  # number of input dimensions\n",
    "    )\n",
    "    \n",
    "    # Step 3: Combining the decoder with the Segment Anything backbone for automatic instance segmentation.\n",
    "    ais = InstanceSegmentationWithDecoder(predictor, decoder)\n",
    "    \n",
    "    # Step 4: Initializing the precomputed image embeddings to perform faster automatic instance segmentation.\n",
    "    ais.initialize(\n",
    "        image=image,  # the input image\n",
    "        image_embeddings=image_embeddings,  # precomputed image embeddings\n",
    "    )\n",
    "\n",
    "    # Step 5: Getting automatic instance segmentations for the given image and applying the relevant post-processing steps.\n",
    "    prediction = ais.generate()\n",
    "    prediction = mask_data_to_segmentation(prediction, with_background=True)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa49ad",
   "metadata": {},
   "source": [
    "## Locate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900ba1e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select folders for training\n",
    "train_dir = ''\n",
    "segmentation_dir = ''\n",
    "\n",
    "# Select separate folders for validation data\n",
    "val_dir = ''\n",
    "val_segmentation_dir = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d986c",
   "metadata": {},
   "source": [
    "## Create the dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f300d7c",
   "metadata": {},
   "source": [
    "#### First, let's visualize how our images look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98892c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_paths = sorted(glob(os.path.join(train_dir, \"*\")))\n",
    "segmentation_paths = sorted(glob(os.path.join(segmentation_dir, \"*\")))\n",
    "\n",
    "for image_path, segmentation_path in zip(image_paths, segmentation_paths):\n",
    "    image = imageio.imread(image_path)\n",
    "    segmentation = imageio.imread(segmentation_path)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    segmentation = connected_components(segmentation)\n",
    "    ax[1].imshow(segmentation, cmap=get_random_colors(segmentation), interpolation=\"nearest\")\n",
    "    ax[1].set_title(\"Ground Truth Instances\")\n",
    "    ax[1].axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    break  # comment this out in case you want to visualize all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc82cfa",
   "metadata": {},
   "source": [
    "#### Next, let's create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057712cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# torch_em.default_segmentation_loader is a convenience function to build a torch dataloader\n",
    "# from image data and labels for training segmentation models.\n",
    "# It supports image data in various formats. Here, we load image data and labels from the two\n",
    "# folders with tif images that were downloaded by the example data functionality, by specifying\n",
    "# `raw_key` and `label_key` as `*.tif`. This means all images in the respective folders that end with\n",
    "# .tif will be loadded.\n",
    "# The function supports many other file formats. For example, if you have tif stacks with multiple slices\n",
    "# instead of multiple tif images in a foldder, then you can pass raw_key=label_key=None.\n",
    "# For more information, here is the documentation: https://github.com/constantinpape/torch-em/blob/main/torch_em/data/datasets/README.md\n",
    "\n",
    "# Load images from multiple files in folder via pattern (here: all tif files)\n",
    "raw_key, label_key = \"*.tif\", \"*.tif\"\n",
    "\n",
    "# Alternative: if you have tif stacks you can just set raw_key and label_key to None\n",
    "# raw_key, label_key= None, None\n",
    "\n",
    "# The 'roi' argument can be used to subselect parts of the data.\n",
    "# Here, we use it to select the first 70 images (frames) for the train split and the other frames for the val split.\n",
    "\n",
    "#We just load all images for training an validation from different folders\n",
    "train_roi = np.s_[:, :, :]\n",
    "val_roi = np.s_[:, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64d5da",
   "metadata": {},
   "source": [
    "## Label Transform\n",
    "\n",
    "The idea here is to convert the ground-truth to the desired instance for finetuning Segment Anything, and in addition if desired, to learn the foreground and distances to the object centers and object boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f2fda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The script below returns the train or val data loader for finetuning SAM.\n",
    "\n",
    "# The data loader must be a torch data loader that returns `x, y` tensors,\n",
    "# where `x` is the image data and `y` are the labels.\n",
    "# The labels have to be in a label mask instance segmentation format.\n",
    "# i.e. a tensor of the same spatial shape as `x`, with each object mask having its own ID.\n",
    "# Important: the ID 0 is reseved for background, and the IDs must be consecutive\n",
    "\n",
    "# Here, we use `torch_em.default_segmentation_loader` for creating a suitable data loader from\n",
    "# the example hela data. You can either adapt this for your own data or write a suitable torch dataloader yourself.\n",
    "# Here's a quickstart notebook to create your own dataloaders: https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb\n",
    "batch_size = 1  # the training batch size\n",
    "patch_shape = (1, 1024, 1024)  # the size of patches for training\n",
    "\n",
    "# Train an additional convolutional decoder for end-to-end automatic instance segmentation\n",
    "train_instance_segmentation = True\n",
    "\n",
    "label_transform = LabelTransform(train_instance_segmentation)\n",
    "\n",
    "train_loader = torch_em.default_segmentation_loader(\n",
    "    raw_paths=train_dir,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=segmentation_dir,\n",
    "    label_key=label_key,\n",
    "    patch_shape=patch_shape,\n",
    "    batch_size=batch_size,\n",
    "    ndim=2,\n",
    "    is_seg_dataset=True,\n",
    "    rois=train_roi,\n",
    "    label_transform=label_transform,\n",
    "    shuffle=True,\n",
    "    raw_transform=sam_training.identity,\n",
    ")\n",
    "val_loader = torch_em.default_segmentation_loader(\n",
    "    raw_paths=val_dir,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=val_segmentation_dir,\n",
    "    label_key=label_key,\n",
    "    patch_shape=patch_shape,\n",
    "    batch_size=batch_size,\n",
    "    ndim=2,\n",
    "    is_seg_dataset=True,\n",
    "    rois=val_roi,\n",
    "    label_transform=label_transform,\n",
    "    shuffle=True,\n",
    "    raw_transform=sam_training.identity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ccb3c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's check how our samples look from the dataloader\n",
    "check_loader(train_loader, 4, plt=True)\n",
    "check_loader(val_loader, 4, plt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad077c8",
   "metadata": {},
   "source": [
    "### Run the actual model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09332b59",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# All hyperparameters for training.\n",
    "n_objects_per_batch = 5  # the number of objects per batch that will be sampled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # the device/GPU used for training\n",
    "n_epochs = 100  # how long we train (in epochs)\n",
    "\n",
    "# The model_type determines which base model is used to initialize the weights that are finetuned.\n",
    "# We use vit_b here because it can be trained faster. Note that vit_h usually yields higher quality results.\n",
    "model_type = \"vit_l\"\n",
    "\n",
    "# The name of the checkpoint. The checkpoints will be stored in './checkpoints/<checkpoint_name>'\n",
    "checkpoint_name = \"sam_spheroidv2\"\n",
    "\n",
    "\n",
    "# Run training\n",
    "sam_training.train_sam(\n",
    "    name=checkpoint_name,\n",
    "    save_root=os.path.join(root_dir, \"models\"),\n",
    "    model_type=model_type,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=n_epochs,\n",
    "    n_objects_per_batch=n_objects_per_batch,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e48cb8",
   "metadata": {},
   "source": [
    "### Train networks on subset of data\n",
    "\n",
    "This script allows to run the training for the different tumor models in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8fe27-36d3-4800-8a2d-591c45b790c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames = ['CS1-2', 'CS2A', 'CS2B', 'CS5']\n",
    "n_train_images = 12  # Number of training images per model\n",
    "n_val_images = 12    # Number of validation images per model\n",
    "\n",
    "for i, modelname in enumerate(modelnames):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    \n",
    "    with CudaMemoryManager():\n",
    "        train_start = i * n_train_images\n",
    "        train_end = train_start + n_train_images\n",
    "        val_start = i * n_val_images\n",
    "        val_end = val_start + n_val_images\n",
    "        train_roi = (slice(train_start, train_end), slice(None), slice(None))\n",
    "        val_roi = (slice(val_start, val_end), slice(None), slice(None))\n",
    "        \n",
    "        train_loader = torch_em.default_segmentation_loader(\n",
    "            raw_paths=train_dir,\n",
    "            raw_key=raw_key,\n",
    "            label_paths=segmentation_dir,\n",
    "            label_key=label_key,\n",
    "            patch_shape=patch_shape,\n",
    "            batch_size=batch_size,\n",
    "            ndim=2,\n",
    "            is_seg_dataset=True,\n",
    "            rois=train_roi,\n",
    "            label_transform=label_transform,\n",
    "            shuffle=True,\n",
    "            raw_transform=sam_training.identity,\n",
    "        )\n",
    "        \n",
    "        val_loader = torch_em.default_segmentation_loader(\n",
    "            raw_paths=val_dir,\n",
    "            raw_key=raw_key,\n",
    "            label_paths=val_segmentation_dir,\n",
    "            label_key=label_key,\n",
    "            patch_shape=patch_shape,\n",
    "            batch_size=batch_size,\n",
    "            ndim=2,\n",
    "            is_seg_dataset=True,\n",
    "            rois=val_roi,\n",
    "            label_transform=label_transform,\n",
    "            shuffle=True,\n",
    "            raw_transform=sam_training.identity,\n",
    "        )\n",
    "        \n",
    "        checkpoint_name = f'sam_spheroidv2_{modelname}'\n",
    "        \n",
    "        sam_training.train_sam(\n",
    "            name=checkpoint_name,\n",
    "            save_root=os.path.join(root_dir, \"models\"),\n",
    "            model_type=model_type,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            n_epochs=n_epochs,\n",
    "            n_objects_per_batch=n_objects_per_batch,\n",
    "            with_segmentation_decoder=train_instance_segmentation,\n",
    "            device=device,\n",
    "        )\n",
    "    \n",
    "    # Explicitly delete objects\n",
    "    del train_loader, val_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0d761-1cff-48f6-ad4d-23c7a6a06d66",
   "metadata": {},
   "source": [
    "### Export all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca3327-b6d5-47d6-898d-59aa9f018427",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_checkpoint_name = 'sam_spheroidv2'\n",
    "\n",
    "modelnames = ['','CS1-2', 'CS2A', 'CS2B', 'CS5']\n",
    "\n",
    "best_checkpoint = os.path.join(\"models\", \"checkpoints\", checkpoint_name, \"best.pt\")\n",
    "save_path = os.path.join(\"models\", \"export\",\"sam_spheroidv2.zip\")\n",
    "train_instance_segmentation = True  # Set this to True or False based on your setup\n",
    "model_type = \"vit_l_lm\"\n",
    "\n",
    "for model_name in modelnames:\n",
    "    if model_name == '':\n",
    "        checkpoint_name = base_checkpoint_name\n",
    "    else:\n",
    "        checkpoint_name = f\"{base_checkpoint_name}_{model_name}\"\n",
    "    best_checkpoint = os.path.join(\"models\", \"checkpoints\", checkpoint_name, \"best.pt\")\n",
    "    save_path = os.path.join(\"models\", \"export\",f\"{base_checkpoint_name}_{model_name}.zip\")\n",
    "    util.export_custom_sam_model(best_checkpoint,model_type,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59bf3a-5be5-4d1e-834c-09aede4ec5da",
   "metadata": {},
   "source": [
    "## Run automatic instance segmentation (AIS) prediction on folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57a9e2-515d-43a8-83c7-2848c8a0994e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_image_paths(folder_path):\n",
    "    return [\n",
    "        f for f in glob(os.path.join(folder_path, '**', '*.tif'), recursive=True)\n",
    "        if 'mask' not in os.path.basename(f) and 'labelIm' not in os.path.basename(f) and '_with_overlay' not in os.path.basename(f)\n",
    "    ]\n",
    "def merge_overlapping_labels(label_image):\n",
    "    # Check if the image is 2D or 3D\n",
    "    if label_image.ndim == 2:\n",
    "        selem = morphology.disk(1)\n",
    "    elif label_image.ndim == 3:\n",
    "        selem = morphology.ball(1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image dimension. Expected 2D or 3D image.\")\n",
    "\n",
    "    # Dilate each label to find overlaps\n",
    "    dilated = morphology.dilation(label_image, selem)\n",
    "    \n",
    "    # Find overlapping regions\n",
    "    overlap = (dilated > 0) & (label_image == 0)\n",
    "    \n",
    "    # Assign overlapping regions to the nearest label\n",
    "    overlap_labels = segmentation.watershed(image=np.zeros_like(label_image), \n",
    "                                            markers=label_image, \n",
    "                                            mask=dilated > 0)\n",
    "    \n",
    "    # Relabel the image\n",
    "    return label(overlap_labels)\n",
    "\n",
    "def filter_small_labels(label_image, min_size=100):\n",
    "    return morphology.remove_small_objects(label_image, min_size=min_size)\n",
    "\n",
    "def create_outline_roi(label_region):\n",
    "    # Find contours of the region\n",
    "    contours = measure.find_contours(label_region.image, 0.5)\n",
    "    \n",
    "    if contours:\n",
    "        # Use the first contour (usually the outer boundary)\n",
    "        contour = contours[0]\n",
    "        \n",
    "        # Add the offset to the contour coordinates\n",
    "        contour[:, 0] += label_region.bbox[0]\n",
    "        contour[:, 1] += label_region.bbox[1]\n",
    "        \n",
    "        # Swap x and y coordinates to match ImageJ's expectation\n",
    "        contour = contour[:, [1, 0]]\n",
    "        \n",
    "        # Create an outline ROI\n",
    "        roi = ImagejRoi.frompoints(contour.astype(np.float32))\n",
    "        roi.name = f\"Label_{label_region.label}\"\n",
    "        \n",
    "        return roi\n",
    "    \n",
    "    return None\n",
    "\n",
    "def analyze_label_image(label_image):\n",
    "    props = regionprops(label_image)\n",
    "    measurements = []\n",
    "    rois = []\n",
    "    for prop in props:\n",
    "        area = prop.area\n",
    "        perimeter = prop.perimeter\n",
    "        height = prop.bbox[2] - prop.bbox[0]\n",
    "        width = prop.bbox[3] - prop.bbox[1]\n",
    "        diameter = prop.equivalent_diameter\n",
    "        volume = (4/3) * math.pi * (diameter/2)**3  # Assuming spherical shape\n",
    "        circular_area = math.pi*((height+width)/4)**2\n",
    "        circular_volume = (4/3) * math.pi*((height+width)/4)**3 \n",
    "\n",
    "        measurements.append({\n",
    "            'Label': prop.label,\n",
    "            'Area': area,\n",
    "            'Height': height,\n",
    "            'Width': width,\n",
    "            'Perimeter': perimeter,\n",
    "            'Diameter': diameter,\n",
    "            'Volume': volume,\n",
    "            'Circular_Area': circular_area,\n",
    "            'Cicular_Volume': circular_volume\n",
    "        })\n",
    "\n",
    "        # Create ROI for ImageJ\n",
    "        roi = create_outline_roi(prop)\n",
    "        if roi:\n",
    "            rois.append(roi)\n",
    "\n",
    "    return pd.DataFrame(measurements), rois\n",
    "\n",
    "# Main script and input parameters\n",
    "\n",
    "input_folder = \"/run/user/1012/gvfs/smb-share:server=oic-station2,share=oic-admin/Maarten/Genetics/Maayke/Sensitive/\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # the device/GPU used for training\n",
    "base_checkpoint_name = 'sam_spheroidv2'\n",
    "best_checkpoint = os.path.join(\"models\", \"checkpoints\", checkpoint_name, \"best.pt\")\n",
    "train_instance_segmentation = True  # Set this to True or False based on your setup\n",
    "model_type = \"vit_l\"\n",
    "modelnames = ['','CS1-2', 'CS2A', 'CS2B', 'CS5']\n",
    "\n",
    "image_paths = get_image_paths(input_folder)\n",
    "print(f\"Found {len(image_paths)} images.\")\n",
    "\n",
    "for model_name in modelnames:\n",
    "    if model_name == '':\n",
    "        checkpoint_name = base_checkpoint_name\n",
    "    else:\n",
    "        checkpoint_name = f\"{base_checkpoint_name}_{model_name}\"\n",
    "    best_checkpoint = os.path.join(\"models\", \"checkpoints\", checkpoint_name, \"best.pt\")\n",
    "    print(best_checkpoint)\n",
    "    assert os.path.exists(best_checkpoint), \"Please train the model first to run inference on the finetuned model.\"\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"\\nProcessing image {i+1}/{len(image_paths)}:\")\n",
    "        print(f\"File: {image_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the TIFF file\n",
    "            image = tifffile.imread(image_path)\n",
    "            \n",
    "            print(f\"Image shape: {image.shape}\")\n",
    "            print(f\"Image dtype: {image.dtype}\")\n",
    "            \n",
    "            # Run automatic instance segmentation\n",
    "            prediction = run_automatic_instance_segmentation(\n",
    "                image=image,\n",
    "                checkpoint_path=best_checkpoint,\n",
    "                model_type=model_type,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Ensure prediction is a label image\n",
    "            if prediction.dtype != np.int32:\n",
    "                prediction = label(prediction > 0)\n",
    "            \n",
    "            # Merge overlapping labels and filter small objects\n",
    "            try:\n",
    "                merged_prediction = merge_overlapping_labels(prediction)\n",
    "                filtered_prediction = filter_small_labels(merged_prediction, min_size=100)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error in merging or filtering. Using original prediction. Error: {str(e)}\")\n",
    "                filtered_prediction = prediction\n",
    "            \n",
    "            # Visualize the predictions\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            ax[0].imshow(image, cmap=\"gray\")\n",
    "            ax[0].axis(\"off\")\n",
    "            ax[0].set_title(\"Input Image\")\n",
    "            ax[1].imshow(prediction, cmap='nipy_spectral', interpolation=\"nearest\")\n",
    "            ax[1].axis(\"off\")\n",
    "            ax[1].set_title(\"Original Predictions\")\n",
    "            ax[2].imshow(filtered_prediction, cmap='nipy_spectral', interpolation=\"nearest\")\n",
    "            ax[2].axis(\"off\")\n",
    "            ax[2].set_title(\"Merged and Filtered Predictions\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # Save the filtered prediction with \"labelIm_\" prefix\n",
    "            output_filename = f\"labelIm_{model_name}_{os.path.basename(image_path)}\"\n",
    "            output_path = os.path.join(os.path.dirname(image_path), output_filename)\n",
    "            \n",
    "            # Ensure the label image is saved as 16-bit unsigned integer\n",
    "            filtered_prediction_16bit = filtered_prediction.astype(np.uint16)\n",
    "            tifffile.imwrite(output_path, filtered_prediction_16bit, compression='zlib')\n",
    "            print(f\"Saved: {output_path}\")\n",
    "    \n",
    "            # Analyze label image\n",
    "            measurements_df, rois = analyze_label_image(filtered_prediction)\n",
    "    \n",
    "            # Save measurements\n",
    "            measurements_path = os.path.join(os.path.dirname(image_path), f\"measurements_{model_name}_{os.path.basename(image_path)}.csv\")\n",
    "            measurements_df.to_csv(measurements_path, index=False)\n",
    "            print(f\"Saved measurements: {measurements_path}\")\n",
    "    \n",
    "            # Save ROIs\n",
    "            roi_path = os.path.join(os.path.dirname(image_path), f\"ROIs_{model_name}_{os.path.basename(image_path)}.zip\")\n",
    "            roiwrite(roi_path, rois)\n",
    "            print(f\"Saved ROIs: {roi_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "\n",
    "print(\"Done processing images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abe1d7-8c92-47b8-91b2-fb9a54d40d98",
   "metadata": {},
   "source": [
    "## Make plots for all models\n",
    "\n",
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee30800-2bd9-4ecd-97c3-2b8765651b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scaling_info(tumor_folder):\n",
    "    \"\"\"\n",
    "    Read the scaling.csv file from the tumor folder.\n",
    "    Returns a DataFrame with scaling information or None if file is not found.\n",
    "    \"\"\"\n",
    "    scaling_file = os.path.join(tumor_folder, 'scaling.csv')\n",
    "    if os.path.exists(scaling_file):\n",
    "        try:\n",
    "            scaling_df = pd.read_csv(scaling_file)\n",
    "            print(f\"Found scaling information for {os.path.basename(tumor_folder)}\")\n",
    "            return scaling_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading scaling file: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No scaling.csv found in {tumor_folder}. Using default pixel size of 1.\")\n",
    "        return None\n",
    "\n",
    "def apply_scaling(data, scaling_df):\n",
    "    \"\"\"\n",
    "    Apply pixel size scaling to relevant measurements.\n",
    "    \"\"\"\n",
    "    if scaling_df is None:\n",
    "        return data\n",
    "    \n",
    "    scaled_data = data.copy()\n",
    "    \n",
    "    # List of measurements that need scaling\n",
    "    # Linear measurements (multiply by pixel size)\n",
    "    linear_measures = ['Height', 'Width', 'Perimeter', 'Diameter']\n",
    "    # Area measurements (multiply by pixel size squared)\n",
    "    area_measures = ['Area', 'Circular_Area']\n",
    "    # Volume measurements (multiply by pixel size cubed)\n",
    "    volume_measures = ['Volume', 'Circular_Volume']\n",
    "    \n",
    "    # Merge scaling information with data\n",
    "    scaled_data = scaled_data.merge(\n",
    "        scaling_df[['day', 'treatment', 'pixelsize']], \n",
    "        left_on=['Day', 'Treatment'], \n",
    "        right_on=['day', 'treatment'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill any missing scaling values with 1\n",
    "    scaled_data['pixelsize'].fillna(1, inplace=True)\n",
    "    \n",
    "    # Apply scaling to each type of measurement\n",
    "    for measure in linear_measures:\n",
    "        if measure in scaled_data.columns:\n",
    "            scaled_data[measure] = scaled_data[measure] * scaled_data['pixelsize']\n",
    "    \n",
    "    for measure in area_measures:\n",
    "        if measure in scaled_data.columns:\n",
    "            scaled_data[measure] = scaled_data[measure] * (scaled_data['pixelsize'] ** 2)\n",
    "    \n",
    "    for measure in volume_measures:\n",
    "        if measure in scaled_data.columns:\n",
    "            scaled_data[measure] = scaled_data[measure] * (scaled_data['pixelsize'] ** 3)\n",
    "    \n",
    "    # Clean up merged columns\n",
    "    scaled_data.drop(['day', 'treatment', 'pixelsize'], axis=1, inplace=True)\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "def update_plot_labels(measure):\n",
    "    \"\"\"\n",
    "    Return the appropriate unit label based on the measure type.\n",
    "    \"\"\"\n",
    "    linear_measures = ['Height', 'Width', 'Perimeter', 'Diameter']\n",
    "    area_measures = ['Area', 'Circular_Area']\n",
    "    volume_measures = ['Volume', 'Circular_Volume']\n",
    "    \n",
    "    base_name = measure.replace('Normalized_', '')\n",
    "    \n",
    "    if base_name in linear_measures:\n",
    "        return f\"{measure} (µm)\"\n",
    "    elif base_name in area_measures:\n",
    "        return f\"{measure} (µm²)\"\n",
    "    elif base_name in volume_measures:\n",
    "        return f\"{measure} (µm³)\"\n",
    "    else:\n",
    "        return measure\n",
    "\n",
    "# Modify the create_combined_plots function to include units in labels\n",
    "def create_combined_plots(data, output_folder, tumor_model, normalized=False):\n",
    "    measurements = ['Area', 'Height', 'Width', 'Perimeter', 'Diameter', 'Volume', 'Circular_Area', 'Circular_Volume']\n",
    "    if normalized:\n",
    "        measurements = [f'Normalized_{m}' for m in measurements]\n",
    "    days = sorted(data['Day'].unique())\n",
    "    treatments = sorted(data['Treatment'].unique())\n",
    "    ai_models = sorted(data['AI_Model'].unique())\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(ai_models)))\n",
    "\n",
    "    for measure in measurements:\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        positions = np.arange(len(treatments) * len(days))\n",
    "        width = 0.8 / len(ai_models)\n",
    "        \n",
    "        for i, ai_model in enumerate(ai_models):\n",
    "            model_data = data[data['AI_Model'] == ai_model]\n",
    "            \n",
    "            boxplot_positions = []\n",
    "            boxplot_data = []\n",
    "            \n",
    "            for day in days:\n",
    "                for treatment in treatments:\n",
    "                    day_treatment_data = model_data[(model_data['Day'] == day) & (model_data['Treatment'] == treatment)]\n",
    "                    if not day_treatment_data.empty:\n",
    "                        pos = positions[len(treatments) * days.index(day) + treatments.index(treatment)]\n",
    "                        boxplot_positions.append(pos + i*width - 0.4 + width/2)\n",
    "                        boxplot_data.append(day_treatment_data[measure])\n",
    "            \n",
    "            bp = plt.boxplot(boxplot_data,\n",
    "                           positions=boxplot_positions,\n",
    "                           widths=width, patch_artist=True,\n",
    "                           boxprops=dict(facecolor=colors[i], alpha=0.6),\n",
    "                           medianprops=dict(color='black'),\n",
    "                           showfliers=False)\n",
    "            \n",
    "            for j, (day, treatment) in enumerate([(d, t) for d in days for t in treatments]):\n",
    "                day_treatment_data = model_data[(model_data['Day'] == day) & (model_data['Treatment'] == treatment)]\n",
    "                if not day_treatment_data.empty:\n",
    "                    pos = positions[j]\n",
    "                    plt.scatter(np.random.normal(pos + i*width - 0.4 + width/2, 0.01, len(day_treatment_data)),\n",
    "                              day_treatment_data[measure], \n",
    "                              c=[colors[i]], s=20, alpha=0.6)\n",
    "\n",
    "        plt.xlabel('Day and Treatment')\n",
    "        plt.ylabel(update_plot_labels(measure))\n",
    "        plt.title(f'{\"Normalized \" if normalized else \"\"}Distribution of {measure} by AI Model, Day, and Treatment - {tumor_model}')\n",
    "        plt.xticks(positions, [f'D{day} T{treatment}' for day in days for treatment in treatments], rotation=45)\n",
    "        \n",
    "        handles = [plt.Rectangle((0,0),1,1, facecolor=colors[i], edgecolor='none', alpha=0.6) for i in range(len(ai_models))]\n",
    "        plt.legend(handles, ai_models, loc='upper right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{\"normalized_\" if normalized else \"\"}{measure}_combined_{tumor_model}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e38ed-25f3-43fd-a8b7-961f55d8d4a9",
   "metadata": {},
   "source": [
    "### Main execution of the plots functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1afeb-4dbf-4c4d-8cc1-81d05412e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"\"\n",
    "tumor_models = [\"CS1-2\", \"CS2A\", \"CS2B\", \"CS5\"]\n",
    "ai_models = [\"\", \"CS1-2\", \"CS2A\", \"CS2B\", \"CS5\"]  # \"\" represents the model trained on all data\n",
    "\n",
    "for tumor_model in tumor_models:\n",
    "    print(f\"Processing tumor model: {tumor_model}\")\n",
    "    \n",
    "    tumor_folder = os.path.join(base_folder, tumor_model)\n",
    "    if not os.path.exists(tumor_folder):\n",
    "        print(f\"Folder for {tumor_model} not found. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Read scaling information\n",
    "    scaling_df = read_scaling_info(tumor_folder)\n",
    "    \n",
    "    # Read all CSV files for this tumor model\n",
    "    data = read_csv_files(tumor_folder, ai_models)\n",
    "    \n",
    "    if data.empty:\n",
    "        print(f\"No data found for {tumor_model}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Apply scaling to the data\n",
    "    data = apply_scaling(data, scaling_df)\n",
    "    \n",
    "    # Create output folder within the tumor model folder\n",
    "    output_folder = os.path.join(tumor_folder, \"analysis_results\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create combined plots for this tumor model\n",
    "    create_combined_plots(data, output_folder, tumor_model)\n",
    "    create_individual_ai_model_plots(data, output_folder, tumor_model)\n",
    "    \n",
    "    # Normalize data\n",
    "    normalized_data = normalize_to_day0_median(data)\n",
    "    \n",
    "    # Create normalized combined plots for this tumor model\n",
    "    create_combined_plots(normalized_data, output_folder, tumor_model, normalized=True)\n",
    "    create_individual_ai_model_plots(normalized_data, output_folder, tumor_model, normalized=True)\n",
    "    \n",
    "    # Create the reorganized Excel file\n",
    "    create_reorganized_excel(normalized_data, output_folder, tumor_model)\n",
    "    \n",
    "    # Save data to Excel files\n",
    "    for ai_model in ai_models:\n",
    "        model_data = normalized_data[normalized_data['AI_Model'] == (ai_model if ai_model else \"alldata\")]\n",
    "        save_to_excel(model_data, output_folder, tumor_model, ai_model if ai_model else \"alldata\")\n",
    "    \n",
    "    print(f\"Finished processing {tumor_model}\")\n",
    "\n",
    "print(\"All tumor models have been processed. Plots and Excel files are saved in their respective 'analysis_results' folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12480488-dad5-4095-b10f-bf2c8e24c0dd",
   "metadata": {},
   "source": [
    "## Comparison with manual measurements\n",
    "\n",
    "### Function definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b905e-9c7e-4578-8e5f-3bd191f79204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_id(filename):\n",
    "    match = re.search(r'(\\d+)\\.tif\\.csv$', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        print(f\"Warning: Could not extract image ID from filename: {filename}\")\n",
    "        return None\n",
    "\n",
    "def read_csv_files(folder_path, ai_models):\n",
    "    data = []\n",
    "    for ai_model in ai_models:\n",
    "        for day_treatment_folder in sorted(glob(os.path.join(folder_path, 'd*'))):\n",
    "            day, treatment = os.path.basename(day_treatment_folder).split()\n",
    "            day = int(day[1:])\n",
    "            treatment = int(treatment)\n",
    "            \n",
    "            csv_files = glob(os.path.join(day_treatment_folder, f'measurements_{ai_model}_*.csv'))\n",
    "            for file in csv_files:\n",
    "                df = pd.read_csv(file)\n",
    "                df['AI_Model'] = ai_model if ai_model else \"alldata\"\n",
    "                df['Day'] = day\n",
    "                df['Treatment'] = treatment\n",
    "                if 'Circular_Area' not in df.columns:\n",
    "                    df['Circular_Area'] = math.pi*((df['Height']+df['Width'])/4)**2\n",
    "                if 'Circular_Volume' not in df.columns:\n",
    "                    df['Circular_Volume'] = (4/3) * math.pi * ((df['Height']+df['Width'])/4)**3 \n",
    "                image_id = extract_image_id(os.path.basename(file))\n",
    "                if image_id is not None:\n",
    "                    df['Image'] = image_id\n",
    "                    data.append(df)\n",
    "                else:\n",
    "                    print(f\"Skipping file due to invalid image ID: {file}\")\n",
    "    \n",
    "    return pd.concat(data, ignore_index=True) if data else pd.DataFrame()\n",
    "\n",
    "def read_manual_measurements(file_path):\n",
    "    manual_df = pd.read_csv(file_path, sep=\",\")\n",
    "    manual_df = manual_df.dropna(subset=['Vcore'])  # Remove rows with NaN Vcore\n",
    "    return manual_df\n",
    "\n",
    "def average_measurements(df, group_cols, value_cols):\n",
    "    return df.groupby(group_cols)[value_cols].mean().reset_index()\n",
    "\n",
    "def create_scatter_plots(manual_df, ai_df, output_folder, tumor_model):\n",
    "    ai_models = ai_df['AI_Model'].unique()\n",
    "    \n",
    "    manual_avg = average_measurements(manual_df, ['Day', 'Treatment', 'Image'], 'Vcore')\n",
    "    \n",
    "    for ai_model in ai_models:\n",
    "        model_df = ai_df[ai_df['AI_Model'] == ai_model]\n",
    "        \n",
    "        ai_avg = average_measurements(model_df, ['Day', 'Treatment', 'Image'], ['Volume', 'Circular_Volume'])\n",
    "        \n",
    "        merged_df = pd.merge(manual_avg, ai_avg, on=['Day', 'Treatment', 'Image'], how='outer', indicator=True)\n",
    "        \n",
    "        only_manual = merged_df[merged_df['_merge'] == 'left_only']\n",
    "        only_ai = merged_df[merged_df['_merge'] == 'right_only']\n",
    "        \n",
    "        if not only_manual.empty:\n",
    "            print(f\"Warning: The following images are in manual measurements but not in AI predictions for {ai_model}:\")\n",
    "            print(only_manual[['Day', 'Treatment', 'Image']])\n",
    "        \n",
    "        if not only_ai.empty:\n",
    "            print(f\"Warning: The following images are in AI predictions but not in manual measurements for {ai_model}:\")\n",
    "            print(only_ai[['Day', 'Treatment', 'Image']])\n",
    "        \n",
    "        merged_df = merged_df[merged_df['_merge'] == 'both'].drop('_merge', axis=1)\n",
    "        \n",
    "        for ai_measure in ['Volume', 'Circular_Volume']:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            \n",
    "            days = sorted(merged_df['Day'].unique())\n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, len(days)))\n",
    "            for i, day in enumerate(days):\n",
    "                day_data = merged_df[merged_df['Day'] == day]\n",
    "                plt.scatter(day_data['Vcore'], day_data[ai_measure], c=[colors[i]], label=f'Day {day}', alpha=0.6)\n",
    "            \n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(merged_df['Vcore'], merged_df[ai_measure])\n",
    "            line = slope * merged_df['Vcore'] + intercept\n",
    "            plt.plot(merged_df['Vcore'], line, color='r', label=f'R² = {r_value**2:.3f}')\n",
    "            \n",
    "            plt.xlabel('Average Manual Measurement (Vcore)')\n",
    "            plt.ylabel(f'Average AI Prediction ({ai_measure})')\n",
    "            plt.title(f'Average Manual vs AI Measurements Correlation - {tumor_model}, AI Model: {ai_model}, Measure: {ai_measure}')\n",
    "            plt.legend()\n",
    "            \n",
    "            pearson_corr, _ = stats.pearsonr(merged_df['Vcore'], merged_df[ai_measure])\n",
    "            spearman_corr, _ = stats.spearmanr(merged_df['Vcore'], merged_df[ai_measure])\n",
    "            plt.text(0.05, 0.95, f'Pearson r: {pearson_corr:.3f}\\nSpearman ρ: {spearman_corr:.3f}', \n",
    "                     transform=plt.gca().transAxes, verticalalignment='top')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_folder, f'avg_manual_vs_ai_correlation_{ai_model}_{tumor_model}_{ai_measure}.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "def save_summary_to_excel(manual_df, ai_df, output_folder, tumor_model):\n",
    "    # Prepare manual data\n",
    "    manual_avg = average_measurements(manual_df, ['Day', 'Treatment', 'Image'], ['Vcore','hcore','wcore'])\n",
    "    manual_avg = manual_avg.rename(columns={'Vcore': 'Manual_Vcore','hcore': 'Manual_hcore','wcore': 'Manual_wcore'})\n",
    "    \n",
    "\n",
    "    # Prepare AI data\n",
    "    ai_models = ai_df['AI_Model'].unique()\n",
    "    ai_data_list = []\n",
    "\n",
    "    for ai_model in ai_models:\n",
    "        model_df = ai_df[ai_df['AI_Model'] == ai_model]\n",
    "        ai_avg = average_measurements(model_df, ['Day', 'Treatment', 'Image'], ['Height','Width','Perimeter','Diameter', 'Volume', 'Circular_Volume','Area'])\n",
    "        ai_avg = ai_avg.rename(columns={\n",
    "            'Volume': f'{ai_model}_Volume',\n",
    "            'Circular_Volume': f'{ai_model}_Circular_Volume',\n",
    "            'Height': f'{ai_model}_Height',\n",
    "            'Width': f'{ai_model}_Width',\n",
    "            'Perimeter': f'{ai_model}_Perimeter',\n",
    "            'Diameter': f'{ai_model}_Diameter',\n",
    "            'Area': f'{ai_model}_Area'\n",
    "        })\n",
    "        ai_data_list.append(ai_avg)\n",
    "\n",
    "    # Merge all data\n",
    "    merged_data = manual_avg\n",
    "    for ai_data in ai_data_list:\n",
    "        merged_data = pd.merge(merged_data, ai_data, on=['Day', 'Treatment', 'Image'], how='outer')\n",
    "\n",
    "    # Sort the data\n",
    "    merged_data = merged_data.sort_values(by=['Treatment', 'Day', 'Image'])\n",
    "\n",
    "    # Save to Excel\n",
    "    excel_path = os.path.join(output_folder, f'{tumor_model}_summary.xlsx')\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        for treatment in sorted(merged_data['Treatment'].unique()):\n",
    "            treatment_data = merged_data[merged_data['Treatment'] == treatment]\n",
    "            sheet_name = str(treatment)\n",
    "            \n",
    "            # Select all columns except 'Treatment'\n",
    "            columns = [col for col in treatment_data.columns if col != 'Treatment']\n",
    "            \n",
    "            treatment_data[columns].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Excel summary saved to {excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23493b67",
   "metadata": {},
   "source": [
    "### Main execution of the comparison with manual measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab7dc9-eea4-46ed-8120-ec7f5ba8d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"\"\n",
    "tumor_model = \"CS2A\" \n",
    "ai_models = [\"\", \"CS1-2\", \"CS2A\", \"CS2B\", \"CS5\"]  # \"\" represents the model trained on all data\n",
    "\n",
    "tumor_folder = os.path.join(base_folder, tumor_model)\n",
    "if not os.path.exists(tumor_folder):\n",
    "    print(f\"Folder for {tumor_model} not found. Exiting...\")\n",
    "else:\n",
    "    print(f\"Processing tumor model: {tumor_model}\")\n",
    "    \n",
    "    ai_data = read_csv_files(tumor_folder, ai_models)\n",
    "    \n",
    "    if ai_data.empty:\n",
    "        print(f\"No AI prediction data found for {tumor_model}. Exiting...\")\n",
    "    else:\n",
    "        manual_file = os.path.join(tumor_folder, f\"{tumor_model}-manual.csv\")\n",
    "        manual_data = read_manual_measurements(manual_file)\n",
    "        \n",
    "        output_folder = os.path.join(tumor_folder, \"analysis_results\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        create_scatter_plots(manual_data, ai_data, output_folder, tumor_model)\n",
    "        save_summary_to_excel(manual_data, ai_data, output_folder, tumor_model)\n",
    "                    \n",
    "        print(f\"Finished processing {tumor_model}\")\n",
    "\n",
    "print(\"Analysis complete. Plots and Excel summary are saved in the 'analysis_results' folder.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
